{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omshree-16/Personality-Prediction-Model/blob/main/Personality_Prediction_Model_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adpLLSc4c1RS"
      },
      "source": [
        "### Downloading datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmJkOr4kdhmG",
        "outputId": "7f633722-2bc4-481a-da0d-96c0e2968d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbl_paClxi4Q",
        "outputId": "13455d69-13bc-4af9-9d23-d0bd39d440e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./mbti-type\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download('https://www.kaggle.com/datasnaek/mbti-type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44Uj0OnbdujA",
        "outputId": "d3960911-e402-4e06-a73f-8b4d0e297ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: omshree16\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kaggle/meta-kaggle\n"
          ]
        }
      ],
      "source": [
        "od.download('https://www.kaggle.com/kaggle/meta-kaggle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fanpPNiqfELN"
      },
      "source": [
        "### importing all required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA9a20-Iezzh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from time import time\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvqO-EpFf90p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print('mbti-type : ',os.listdir('mbti-type'))\n",
        "print('meta-kaggle : ',os.listdir('meta-kaggle'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Y-7b0cgGOu"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('mbti-type/mbti_1.csv')\n",
        "forum_data = pd.read_csv('meta-kaggle/ForumMessages.csv')\n",
        "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition',\n",
        "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling',\n",
        "        'J':'Judging', 'P': 'Perceiving'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxpzxSzCNxy5"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWT2Jc7oh45W"
      },
      "source": [
        "### Let's view all datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9y8rndHgkFf"
      },
      "outputs": [],
      "source": [
        "print(train_data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfZKBZUgosD"
      },
      "outputs": [],
      "source": [
        "print(forum_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkyNNISiiDDm"
      },
      "source": [
        "### Let's view some info about our trainning dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6KG-9vkhZNP"
      },
      "outputs": [],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQb-MqfSiGtP"
      },
      "outputs": [],
      "source": [
        "type_count = train_data['type'].value_counts()\n",
        "colors = sns.color_palette(\"pastel\")\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=type_count.index, y=type_count.values, alpha=0.8, palette=colors)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Types', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIexqz7Uih8y"
      },
      "source": [
        "### Handle missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHPCgJc0ii8C"
      },
      "outputs": [],
      "source": [
        "##ForumMessages.csv\n",
        "print('Forum Missing Values:')\n",
        "print(forum_data.isnull().sum())\n",
        "\n",
        "##mbti_1.csv\n",
        "print('Training Missing Values:')\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "forum_data['Message'] = forum_data['Message'].fillna('')\n",
        "\n",
        "print('Forum Missing Values:')\n",
        "print(forum_data.isnull().sum())\n",
        "\n",
        "print(forum_data['PostUserId'].value_counts())\n",
        "\n",
        "forum_data_agg = forum_data.groupby('PostUserId')['Message'].agg(lambda col: ' '.join(col)).reset_index()\n",
        "print(forum_data_agg['PostUserId'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIxPYVNkk_NK"
      },
      "source": [
        "### Cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWELY19k-Va"
      },
      "outputs": [],
      "source": [
        "#function to clean data\n",
        "def clean_text(text):\n",
        "    #get rid of html and seperators\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r'  ', text)\n",
        "    text = re.sub(r'http\\S+', r'  ', text)\n",
        "    #get rid of punctuation\n",
        "    text = text.replace('.', '  ')\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    #get rid of numbers\n",
        "    text = ''.join(i for i in text if not i.isdigit())\n",
        "    return text\n",
        "\n",
        "train_data['clean_posts'] = train_data['posts'].apply(clean_text)\n",
        "train_data['clean_posts'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXSUclflese"
      },
      "outputs": [],
      "source": [
        "forum_data_agg['clean_messages'] = forum_data_agg['Message'].apply(clean_text)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeaZfb-4lju2"
      },
      "outputs": [],
      "source": [
        "#function to split string by uppercase\n",
        "def split_uppercase(text):\n",
        "    text_list = text.split()\n",
        "    new_list = []\n",
        "    for i in text_list:\n",
        "        if i.isupper() == False: #don't split acronyms\n",
        "            word = re.sub(r'([A-Z])', r' \\1', i)\n",
        "            new_list.append(word)\n",
        "        else:\n",
        "            word = i\n",
        "            new_list.append(word)\n",
        "    words = ' '.join(new_list)\n",
        "    return words\n",
        "\n",
        "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(split_uppercase)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cTldeA0lvNo"
      },
      "outputs": [],
      "source": [
        "#function to stem words\n",
        "def stem_text(text):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    words_list = text.split()\n",
        "    new_list = []\n",
        "    for i in words_list:\n",
        "        word = stemmer.stem(i)\n",
        "        new_list.append(word)\n",
        "\n",
        "    words = new_list\n",
        "    words = ' '.join(words)\n",
        "    return words\n",
        "train_data['clean_posts'] = train_data['clean_posts'].apply(stem_text)\n",
        "train_data['clean_posts'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECz5cjzcmALx"
      },
      "outputs": [],
      "source": [
        "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(stem_text)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TahRqesVnhFD"
      },
      "source": [
        "# **Classification(Classifier Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mWqiNrrnkoD"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F-e4N9Rnm8k"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "scoring = {'acc': 'accuracy',\n",
        "           'neg_log_loss': 'neg_log_loss',\n",
        "           'f1_micro': 'f1_micro'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjeZ8n8YsD0F"
      },
      "source": [
        "## ExtraTreesClassifier with SVD(single value decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQSeiytUsL5G"
      },
      "outputs": [],
      "source": [
        "etc = ExtraTreesClassifier(n_estimators = 20, max_depth=4, n_jobs = -1)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "model = Pipeline([('tfidf1', tfidf), ('tsvd1', tsvd), ('etc', etc)])\n",
        "\n",
        "\n",
        "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "results = cross_validate(model, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24HTJCmwsdlb"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_acc']),\n",
        "                                                  np.std(results['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_f1_micro']),\n",
        "                                            np.std(results['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yxCdMnsmtv"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbGS0rWsnoa"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "tfidf2 = CountVectorizer(ngram_range=(1, 1),\n",
        "                         stop_words='english',\n",
        "                         lowercase = True,\n",
        "                         max_features = 5000)\n",
        "\n",
        "model_nb = Pipeline([('tfidf1', tfidf2), ('nb', MultinomialNB())])\n",
        "\n",
        "results_nb = cross_validate(model_nb, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSsy9w7uswS5"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_acc']),\n",
        "                                                  np.std(results_nb['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_f1_micro']),\n",
        "                                            np.std(results_nb['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_nb['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results_nb['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS7MGd9_sykB"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km0Ie3wqszZS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "tfidf2 = CountVectorizer(ngram_range=(1, 1), stop_words='english', lowercase = True, max_features = 5000)\n",
        "\n",
        "model_lr = Pipeline([('tfidf1', tfidf2), ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005))])\n",
        "\n",
        "results_lr = cross_validate(model_lr, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsTIayh1s2oT"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_acc']),\n",
        "                                                  np.std(results_lr['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_f1_micro']),\n",
        "                                            np.std(results_lr['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_lr['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results_lr['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNwtvPn0tXAW"
      },
      "source": [
        "# **Visualization**\n",
        "As our Last model `(Logistic Regression)` gives high accuracy so we will apply our last model to whole users comments.\n",
        "\n",
        "Let's see what is the most common user personalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cdLwBwQt9Is"
      },
      "outputs": [],
      "source": [
        "model_lr.fit(train_data['clean_posts'], train_data['type'])\n",
        "pred_all = model_lr.predict(forum_data_agg['clean_messages'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tato0ax3zFF2"
      },
      "outputs": [],
      "source": [
        "cnt_all = np.unique(pred_all, return_counts=True)\n",
        "\n",
        "pred_df = pd.DataFrame({'personality': cnt_all[0], 'count': cnt_all[1]},\n",
        "                      columns=['personality', 'count'], index=None)\n",
        "\n",
        "pred_df.sort_values('count', ascending=False, inplace=True)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=pred_df['personality'], y=pred_df['count'], alpha=0.8)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Personality', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qi2PxGszTtp"
      },
      "outputs": [],
      "source": [
        "pred_df['percent'] = pred_df['count']/pred_df['count'].sum()\n",
        "pred_df['description'] = pred_df['personality'].apply(lambda x: ' '.join([mbti[l] for l in list(x)]))\n",
        "pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pFEcPibzY0I"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "labels = pred_df['description']\n",
        "sizes = pred_df['percent']*100\n",
        "\n",
        "trace = go.Pie(labels=labels, values=sizes)\n",
        "layout = go.Layout(title='Kaggle Personality Distribution')\n",
        "\n",
        "data = [trace]\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuYpn0rfNFXQpv4wXvmtmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}